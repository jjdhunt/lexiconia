{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import chat_system_messages as sm\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "import asyncio\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # Otherwise use: api_key=\"Your_API_Key\",\n",
    ")\n",
    "\n",
    "def get_completion(comment, system_message, model_class='gpt-3.5', temperature=0, frequency_penalty=1, force_json:bool=True):\n",
    "\n",
    "    models = {'gpt-4': 'gpt-4-0125-preview',\n",
    "              'gpt-3.5': 'gpt-3.5-turbo-0125'}#'gpt-3.5-turbo-1106'}\n",
    "\n",
    "    input_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.01,\n",
    "                                'gpt-3.5-turbo-0125': 0.0005}\n",
    "    output_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.03,\n",
    "                                'gpt-3.5-turbo-0125': 0.0015}\n",
    "\n",
    "    model = models[model_class]\n",
    "    input_price = input_cost_per_1k_tokens[model]\n",
    "    output_price = output_cost_per_1k_tokens[model]\n",
    "\n",
    "    if force_json:\n",
    "        response_format = { \"type\": \"json_object\" }\n",
    "    else:\n",
    "        response_format = None\n",
    "    completion = client.chat.completions.create(model=model,\n",
    "                                                temperature=temperature,\n",
    "                                                frequency_penalty=frequency_penalty,\n",
    "                                                messages=[\n",
    "                                                    {\"role\": \"system\", \"content\": system_message},\n",
    "                                                    {\"role\": \"user\", \"content\": comment}\n",
    "                                                ],\n",
    "                                                response_format=response_format\n",
    "                                                )\n",
    "\n",
    "    # print(f'Got {completion.usage} for \"{comment}\".')\n",
    "    input_cost = input_price * completion.usage.prompt_tokens / 1000\n",
    "    output_cost = output_price * completion.usage.completion_tokens / 1000\n",
    "    return completion.choices[0].message.content, completion.choices[0].message.tool_calls, input_cost + output_cost\n",
    "\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "async def get_completion_async(comment, system_message, model_class='gpt-3.5', temperature=0, frequency_penalty=1, force_json:bool=True):\n",
    "\n",
    "    models = {'gpt-4': 'gpt-4-0125-preview',\n",
    "              'gpt-3.5': 'gpt-3.5-turbo-0125'}#'gpt-3.5-turbo-1106'}\n",
    "\n",
    "    input_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.01,\n",
    "                                'gpt-3.5-turbo-0125': 0.0005}\n",
    "    output_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.03,\n",
    "                                'gpt-3.5-turbo-0125': 0.0015}\n",
    "\n",
    "    model = models[model_class]\n",
    "    input_price = input_cost_per_1k_tokens[model]\n",
    "    output_price = output_cost_per_1k_tokens[model]\n",
    "\n",
    "    if force_json:\n",
    "        response_format = { \"type\": \"json_object\" }\n",
    "    else:\n",
    "        response_format = None\n",
    "    completion = await async_client.chat.completions.create(model=model,\n",
    "                                                temperature=temperature,\n",
    "                                                frequency_penalty=frequency_penalty,\n",
    "                                                messages=[\n",
    "                                                    {\"role\": \"system\", \"content\": system_message},\n",
    "                                                    {\"role\": \"user\", \"content\": comment}\n",
    "                                                ],\n",
    "                                                response_format=response_format\n",
    "                                                )\n",
    "\n",
    "    # print(f'Got {completion.usage} for \"{comment}\".')\n",
    "    input_cost = input_price * completion.usage.prompt_tokens / 1000\n",
    "    output_cost = output_price * completion.usage.completion_tokens / 1000\n",
    "    return completion.choices[0].message.content, completion.choices[0].message.tool_calls, input_cost + output_cost\n",
    "\n",
    "def convert_entry_to_dataframe(entry):\n",
    "    \n",
    "    dictionary = json.loads(entry)\n",
    "    if not dictionary: return\n",
    "\n",
    "    df = pd.DataFrame(dictionary[\"definitions\"])\n",
    "    df.insert(0, \"variants\", ', '.join(dictionary[\"variants\"]))\n",
    "    df.insert(0, \"root_word\", dictionary[\"root_word\"])\n",
    "    df.insert(0, \"term\", dictionary[\"term\"])\n",
    "    return df\n",
    "\n",
    "def get_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words list\n",
    "with open('../google-10000-english-master/20k.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "words = text.split()\n",
    "print(f'There are {len(words):,} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'bowl'\n",
    "model = 'gpt-3.5'\n",
    "entry, _, cost = await get_completion_async(comment=test_word, system_message=sm.samuel_johnson_2, frequency_penalty=0.1, model_class=model)\n",
    "print(f'{model} entry (cost: ${cost:0.4f}):')\n",
    "convert_entry_to_dataframe(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get definitons for every word in the words list\n",
    "# Each get takes awhile, but mulitple can be requested at once, so long as we don't hit the rate limit\n",
    "async def gather_results(my_list):\n",
    "    tasks = [get_completion_async(comment=v, system_message=sm.samuel_johnson_2, frequency_penalty=0.1) for v in my_list]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "batch_size = 200\n",
    "results = []\n",
    "for chunk in get_chunks(words, batch_size):\n",
    "    print('getting another batch...')\n",
    "    result = await gather_results(chunk)\n",
    "    print(f'got a batch of {len(result)} results!')\n",
    "    time.sleep(20)\n",
    "    results += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert json entries to dataframes and concate into one df.\n",
    "# Also summ up the total cost\n",
    "df_list = []\n",
    "total_cost = 0\n",
    "for result in results:\n",
    "    entry, _, cost = result\n",
    "    total_cost += cost\n",
    "    try:\n",
    "        df_list.append(convert_entry_to_dataframe(entry))\n",
    "    except:\n",
    "        print(f'There was a failure for:\\n{entry}')\n",
    "\n",
    "print(f'total cost: ${total_cost:0.4f}:')\n",
    "df = pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "print(f'There are {len(df.term.unique())} unique terms.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df[['term','definition']].apply(lambda row: f'{row.term}: {row.definition}', axis=1).to_list()\n",
    "\n",
    "chunk_size = 2048 #2048 is max batch size of text-embedding-3-small model\n",
    "embeddings = []\n",
    "for chunk in get_chunks(text_list, chunk_size): #2048 is max batch size of text-embedding-3-small model\n",
    "    response = client.embeddings.create(input = chunk, model=\"text-embedding-3-small\")\n",
    "    chunk_embeddings = [d.embedding for d in response.data]\n",
    "    embeddings += chunk_embeddings\n",
    "\n",
    "df['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything above here is time consuming and expensive. Just load a precalculated dataset, if you have one, to modify the UMAP projection and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../semantics_2024-02-07.parquet')\n",
    "df[\"cluster\"] = df[\"cluster\"].astype('category')\n",
    "df[\"valence\"] = df[\"valence\"].astype('category')\n",
    "df[\"physicality\"] = df[\"physicality\"].astype('category')\n",
    "df[\"humanity\"] = df[\"humanity\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP projection\n",
    "random.seed(42)\n",
    "reducer = umap.UMAP(n_neighbors=50, min_dist=0.0)\n",
    "embedding = reducer.fit_transform(df['embedding'].to_list())\n",
    "\n",
    "df['umap_1'] = embedding[:, 0]\n",
    "df['umap_2'] = embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection\n",
    "with open('../data/umap.pkl', 'wb') as file:\n",
    "    pickle.dump(reducer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load projection\n",
    "with open('../data/umap.pkl', 'rb') as file:\n",
    "    reducer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the points, in umap projection, using dbscan. These will be the Lands.\n",
    "random.seed(42)\n",
    "dbscan = DBSCAN(eps=0.08, min_samples=30)  # Adjust eps and min_samples as needed\n",
    "df['cluster'] = dbscan.fit_predict(df[['umap_1', 'umap_2']].to_numpy())\n",
    "\n",
    "len(df[\"cluster\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use kmeans to cluster the points that were not clustered by dbscan (cluster ID was -1). These will be the Seas.\n",
    "# To distinguish these sea clusters from land clusters, we will give them negative cluster IDs.\n",
    "# And since the clusters IDs will start at zero, subtract one to start at -1.\n",
    "# Number of clusters picked by trial and error.\n",
    "unclustered = df['cluster']==-1\n",
    "\n",
    "kmeans = KMeans(n_clusters=75, random_state=0)\n",
    "df.loc[unclustered, 'cluster'] = -kmeans.fit_predict(df.loc[unclustered, ['umap_1', 'umap_2']].to_numpy()) - 1\n",
    "\n",
    "df[\"cluster\"] = df[\"cluster\"].astype('category')\n",
    "len(df[\"cluster\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Land clusters \n",
    "df['annotation'] = df[['term','definition']].apply(lambda row: f'{row.term}: {row.definition}', axis=1)\n",
    "\n",
    "color_map = {cluster: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, cluster in enumerate(df.cluster.unique())}\n",
    "for key in color_map:\n",
    "    if key < 0:\n",
    "        color_map[key] = 'grey'\n",
    "\n",
    "fig = px.scatter(df, x='umap_1', y='umap_2',\n",
    "                 color='cluster',\n",
    "                 hover_name='annotation',\n",
    "                 hover_data={'umap_1': False, 'umap_2': False},\n",
    "                 title='Land Clusters',\n",
    "                 color_discrete_map=color_map)\n",
    "\n",
    "fig.update_traces(marker={'size': 2.5})\n",
    "fig.update_layout(width=900,\n",
    "                  height=700,\n",
    "                  plot_bgcolor='white',\n",
    "                  showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sea clusters \n",
    "df['annotation'] = df[['term','definition']].apply(lambda row: f'{row.term}: {row.definition}', axis=1)\n",
    "\n",
    "color_map = {cluster: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, cluster in enumerate(df.cluster.unique())}\n",
    "for key in color_map:\n",
    "    if key >= 0:\n",
    "        color_map[key] = 'grey'\n",
    "\n",
    "fig = px.scatter(df, x='umap_1', y='umap_2',\n",
    "                 color='cluster',\n",
    "                 hover_name='annotation',\n",
    "                 hover_data={'umap_1': False, 'umap_2': False},\n",
    "                 title='Sea Clusters',\n",
    "                 color_discrete_map=color_map)\n",
    "\n",
    "fig.update_traces(marker={'size': 2.5})\n",
    "fig.update_layout(width=900,\n",
    "                  height=700,\n",
    "                  plot_bgcolor='white',\n",
    "                  showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic of each cluster from gpt\n",
    "def custom_function(group):\n",
    "    terms = ', '.join(list(group['term'].unique()))\n",
    "    topic, _, cost = get_completion(comment=terms, system_message=sm.identify_topic, force_json=False, model_class='gpt-3.5')\n",
    "    group['cluster_topic'] = topic\n",
    "    return group\n",
    "\n",
    "df = df.groupby('cluster', observed=True).apply(custom_function)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the valence of each cluster from gpt\n",
    "def custom_function(group):\n",
    "    terms = ', '.join(list(group['term'].unique()))\n",
    "    if group['cluster'].to_list()[0]>=0:\n",
    "        score, _, cost = get_completion(comment=terms, system_message=sm.words_valence, force_json=True, model_class='gpt-4')\n",
    "        print(f'cost: {cost}')\n",
    "        try:\n",
    "            score = json.loads(score)\n",
    "            print(group['cluster_topic'].to_list()[0])\n",
    "        except:\n",
    "            print(score)\n",
    "            score = {'valence': -1, 'physicality': -1, 'humanity': -1}\n",
    "    else: # we won't get valences for the seas\n",
    "        score = {'valence': 3, 'physicality': 3, 'humanity': 3}\n",
    "\n",
    "    try: #gpt can fail to return json with the expected keys\n",
    "        group['valence'] = score['valence']\n",
    "        group['physicality'] = score['physicality']\n",
    "        group['humanity'] = score['humanity']\n",
    "    except:\n",
    "        group['valence'] = 3\n",
    "        group['physicality'] = 3\n",
    "        group['humanity'] = 3\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby('cluster', observed=True).apply(custom_function)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df[\"cluster\"] = df[\"cluster\"].astype('category')\n",
    "df[\"valence\"] = df[\"valence\"].astype('category')\n",
    "df[\"physicality\"] = df[\"physicality\"].astype('category')\n",
    "df[\"humanity\"] = df[\"humanity\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a fun name for the region of each cluster from gpt\n",
    "is_land = df.cluster.astype(int)>=0\n",
    "land_topics = df[is_land].cluster_topic.unique()\n",
    "topic_to_land_name = {topic: get_completion(comment=topic, system_message=sm.land_name, force_json=False, temperature=0.2, model_class='gpt-3.5')[0] for topic in land_topics}\n",
    "\n",
    "is_sea = df.cluster.astype(int)<0\n",
    "sea_topics = df[is_sea].cluster_topic.unique()\n",
    "topic_to_sea_name = {topic: get_completion(comment=topic, system_message=sm.sea_name, force_json=False, temperature=0.2, model_class='gpt-3.5')[0] for topic in sea_topics}\n",
    "\n",
    "df.loc[is_land, 'territory_name'] = df.loc[is_land, 'cluster_topic'].map(topic_to_land_name)\n",
    "df.loc[is_sea, 'territory_name'] = df.loc[is_sea, 'cluster_topic'].map(topic_to_sea_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a cluster's territory\n",
    "# df.loc[df.cluster==147, 'territory_name'] = 'The Land of Crisis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../data/semantics_2024-02-07-2.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
