{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import lexiconia.system_messages as sm\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "import asyncio\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the OpenAI clent and define some helper functions\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # Otherwise use: api_key=\"Your_API_Key\",\n",
    ")\n",
    "\n",
    "def get_completion(comment, system_message, model_class='gpt-3.5', temperature=0, frequency_penalty=1, force_json:bool=True):\n",
    "\n",
    "    models = {'gpt-4': 'gpt-4-0125-preview',\n",
    "              'gpt-3.5': 'gpt-3.5-turbo-0125'}\n",
    "\n",
    "    input_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.01,\n",
    "                                'gpt-3.5-turbo-0125': 0.0005}\n",
    "    output_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.03,\n",
    "                                'gpt-3.5-turbo-0125': 0.0015}\n",
    "\n",
    "    model = models[model_class]\n",
    "    input_price = input_cost_per_1k_tokens[model]\n",
    "    output_price = output_cost_per_1k_tokens[model]\n",
    "\n",
    "    if force_json:\n",
    "        response_format = { \"type\": \"json_object\" }\n",
    "    else:\n",
    "        response_format = None\n",
    "    completion = client.chat.completions.create(model=model,\n",
    "                                                temperature=temperature,\n",
    "                                                frequency_penalty=frequency_penalty,\n",
    "                                                messages=[\n",
    "                                                    {\"role\": \"system\", \"content\": system_message},\n",
    "                                                    {\"role\": \"user\", \"content\": comment}\n",
    "                                                ],\n",
    "                                                response_format=response_format\n",
    "                                                )\n",
    "    input_cost = input_price * completion.usage.prompt_tokens / 1000\n",
    "    output_cost = output_price * completion.usage.completion_tokens / 1000\n",
    "    return completion.choices[0].message.content, completion.choices[0].message.tool_calls, input_cost + output_cost\n",
    "\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "async def get_completion_async(comment, system_message, model_class='gpt-3.5', temperature=0, frequency_penalty=1, force_json:bool=True):\n",
    "\n",
    "    models = {'gpt-4': 'gpt-4-0125-preview',\n",
    "              'gpt-3.5': 'gpt-3.5-turbo-0125'}\n",
    "\n",
    "    input_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.01,\n",
    "                                'gpt-3.5-turbo-0125': 0.0005}\n",
    "    output_cost_per_1k_tokens = {'gpt-4-0125-preview': 0.03,\n",
    "                                'gpt-3.5-turbo-0125': 0.0015}\n",
    "\n",
    "    model = models[model_class]\n",
    "    input_price = input_cost_per_1k_tokens[model]\n",
    "    output_price = output_cost_per_1k_tokens[model]\n",
    "\n",
    "    if force_json:\n",
    "        response_format = { \"type\": \"json_object\" }\n",
    "    else:\n",
    "        response_format = None\n",
    "    completion = await async_client.chat.completions.create(model=model,\n",
    "                                                temperature=temperature,\n",
    "                                                frequency_penalty=frequency_penalty,\n",
    "                                                messages=[\n",
    "                                                    {\"role\": \"system\", \"content\": system_message},\n",
    "                                                    {\"role\": \"user\", \"content\": comment}\n",
    "                                                ],\n",
    "                                                response_format=response_format\n",
    "                                                )\n",
    "    input_cost = input_price * completion.usage.prompt_tokens / 1000\n",
    "    output_cost = output_price * completion.usage.completion_tokens / 1000\n",
    "    return completion.choices[0].message.content, completion.choices[0].message.tool_calls, input_cost + output_cost\n",
    "\n",
    "def convert_entry_to_dataframe(entry):\n",
    "    \n",
    "    dictionary = json.loads(entry)\n",
    "    if not dictionary: return\n",
    "\n",
    "    df = pd.DataFrame(dictionary[\"definitions\"])\n",
    "    df.insert(0, \"variants\", ', '.join(dictionary[\"variants\"]))\n",
    "    df.insert(0, \"root_word\", dictionary[\"root_word\"])\n",
    "    df.insert(0, \"term\", dictionary[\"term\"])\n",
    "    return df\n",
    "\n",
    "def get_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of words to use.\n",
    "# To reproduce published results use all 20k words\n",
    "n_words_to_use = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words list\n",
    "with open('20k.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "words = text.split()\n",
    "words = words[:n_words_to_use]\n",
    "print(f'There are {len(words):,} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out getting defintions for a single word\n",
    "# Here you can compare defintions provided by gpt-3.5 and gpt-4. gpt-3.5 provides satisfactory defintions (maybe even superior defintions because gpt-4 can get a little too creative/detailed)\n",
    "test_word = 'tree'\n",
    "model = 'gpt-3.5' # 'gpt-4'\n",
    "entry, _, cost = await get_completion_async(comment=test_word, system_message=sm.samuel_johnson, frequency_penalty=0.1, model_class=model)\n",
    "print(f'{model} entry (cost: ${cost:0.4f}):')\n",
    "convert_entry_to_dataframe(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get definitons for every word in the words list\n",
    "# Each get takes awhile, but mulitple can be requested at once by using async operations, so long as we don't hit our rate limit.\n",
    "# Your rate limit may differ depending on how much you spend on the OpenAI API per month (https://platform.openai.com/docs/guides/rate-limits/usage-tiers)\n",
    "\n",
    "async def gather_results(my_list):\n",
    "    # WARNING: Using gpt-4 here to get defintions for all 20k words could cost hundreds of dollars (and will take days)!\n",
    "    # gpt-3.5 is much cheaper and its definitions are satisfactory\n",
    "    tasks = [get_completion_async(comment=v, system_message=sm.samuel_johnson, frequency_penalty=0.1, model_class='gpt-3.5') for v in my_list]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# batch_size=200 and sleep_time=20 was chosen to not hit the rate limit.\n",
    "# If you hit your rate limit, either decrease batch_size or increase sleep_time\n",
    "batch_size = 200\n",
    "sleep_time = 20\n",
    "results = []\n",
    "for chunk in get_chunks(words, batch_size):\n",
    "    print('getting another batch...')\n",
    "    result = await gather_results(chunk)\n",
    "    print(f'got a batch of {len(result)} results!')\n",
    "    time.sleep(sleep_time)\n",
    "    results += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert json entries to dataframes and concate into one df.\n",
    "# Also summ up the total cost\n",
    "df_list = []\n",
    "total_cost = 0\n",
    "for result in results:\n",
    "    entry, _, cost = result\n",
    "    total_cost += cost\n",
    "    try:\n",
    "        df_list.append(convert_entry_to_dataframe(entry))\n",
    "    except:\n",
    "        print(f'There was a failure for:\\n{entry}')\n",
    "\n",
    "print(f'Total cost: ${total_cost:0.4f}:')\n",
    "df = pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "print(f'There are {len(df.term.unique())} unique terms.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for each word-definition pair.\n",
    "# The exact text that is embedded has the form:\n",
    "# <WORD>: <DEFINITION>\n",
    "text_list = df[['term','definition']].apply(lambda row: f'{row.term}: {row.definition}', axis=1).to_list()\n",
    "\n",
    "chunk_size = 2048 #2048 is max batch size of text-embedding-3-small model\n",
    "embeddings = []\n",
    "for chunk in get_chunks(text_list, chunk_size):\n",
    "    response = client.embeddings.create(input = chunk, model=\"text-embedding-3-small\")\n",
    "    chunk_embeddings = [d.embedding for d in response.data]\n",
    "    embeddings += chunk_embeddings\n",
    "\n",
    "df['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df.to_parquet('semantics.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything above here is time consuming and expensive. Just load a precalculated dataset, if you have one, to modify the UMAP projection and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('semantics_2024-02-07.parquet')\n",
    "# df = pd.read_parquet('semantics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP projection\n",
    "# Projecting 41207 word defintion embeddings takes approximately 30s on Macbook Air/M2 SoC.\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=50, min_dist=0.0, random_state=42)\n",
    "embedding = reducer.fit_transform(df['embedding'].to_list())\n",
    "\n",
    "df['umap_1'] = embedding[:, 0]\n",
    "df['umap_2'] = embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection\n",
    "with open('umap.pkl', 'wb') as file:\n",
    "    pickle.dump(reducer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load projection\n",
    "with open('umap.pkl', 'rb') as file:\n",
    "    reducer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the points, in the 2D umap projection space, using DBSCAN.\n",
    "# These clusters will be the 'Lands'.\n",
    "# The parameters of DBSCAN were choosen by hand to approximately maximize the number of clusters\n",
    "# DBSCAN does not necessarily cluster all points. It finds 'high density' groups of points \n",
    "# where 'high' means at least min_samples points are within eps distance of each other. \n",
    "# DBSCAN is deterministic so no random sed/state needs to be specified to get reproducable results.\n",
    "\n",
    "# hand-tuned values to ~maximize number of clusters when n_words_to_use is 20k\n",
    "if n_words_to_use == 20000:\n",
    "    eps = 0.07\n",
    "    min_samples = 27\n",
    "else: # pick your own values\n",
    "    eps = 0.07\n",
    "    min_samples = int(27 * (n_words_to_use/20000)**0.5)\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['cluster'] = dbscan.fit_predict(df[['umap_1', 'umap_2']].to_numpy())\n",
    "n_lands = len(df['cluster'].unique()) - 1 #minus one because the -1 cluster is not a real cluster, it is the unclustered points\n",
    "print(f\"There are {n_lands} 'Land' clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kmeans to cluster the points, again in the 2D umap projection space, that were not clustered by dbscan (cluster ID was -1).\n",
    "# These clusters will be the 'Seas'.\n",
    "# To distinguish these sea clusters from land clusters, we will give them negative cluster IDs, and since \n",
    "# the clusters IDs will start at zero, subtract one to start at -1.\n",
    "# The number of clusters was picked by trial and error such that there is roughly one sea between each group of lands, so roughly n_lands/3\n",
    "n_seas = max(1, int(n_lands/3))\n",
    "unclustered = df['cluster']==-1\n",
    "kmeans = KMeans(n_clusters=n_seas, random_state=42)\n",
    "df.loc[unclustered, 'cluster'] = -kmeans.fit_predict(df.loc[unclustered, ['umap_1', 'umap_2']].to_numpy()) - 1\n",
    "print(f\"There are {len(df[df['cluster']<0]['cluster'].unique())} 'Sea' clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert clusters to catagorical so plotly uses a qualittative colormap\n",
    "df[\"cluster\"] = df[\"cluster\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Land clusters \n",
    "df['annotation'] = df[['term','definition']].apply(lambda row: f'{row.term}: {row.definition}', axis=1)\n",
    "\n",
    "color_map = {cluster: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, cluster in enumerate(df.cluster.unique())}\n",
    "for key in color_map:\n",
    "    if key < 0:\n",
    "        color_map[key] = 'grey'\n",
    "\n",
    "fig = px.scatter(df, x='umap_1', y='umap_2',\n",
    "                 color='cluster',\n",
    "                 hover_name='annotation',\n",
    "                 hover_data={'umap_1': False, 'umap_2': False},\n",
    "                 title='Land Clusters',\n",
    "                 color_discrete_map=color_map)\n",
    "\n",
    "fig.update_traces(marker={'size': 2.5})\n",
    "fig.update_layout(width=900,\n",
    "                  height=700,\n",
    "                  plot_bgcolor='white',\n",
    "                  showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "df.drop(columns=['annotation'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sea clusters \n",
    "df['annotation'] = df[['term','definition']].apply(lambda row: f'{row.term}: {row.definition}', axis=1)\n",
    "\n",
    "color_map = {cluster: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, cluster in enumerate(df.cluster.unique())}\n",
    "for key in color_map:\n",
    "    if key >= 0:\n",
    "        color_map[key] = 'grey'\n",
    "\n",
    "fig = px.scatter(df, x='umap_1', y='umap_2',\n",
    "                 color='cluster',\n",
    "                 hover_name='annotation',\n",
    "                 hover_data={'umap_1': False, 'umap_2': False},\n",
    "                 title='Sea Clusters',\n",
    "                 color_discrete_map=color_map)\n",
    "\n",
    "fig.update_traces(marker={'size': 2.5})\n",
    "fig.update_layout(width=900,\n",
    "                  height=700,\n",
    "                  plot_bgcolor='white',\n",
    "                  showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "df.drop(columns=['annotation'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic of each cluster from gpt\n",
    "# We will use gpt-3.5 here because it is faster and cheaper than gpt-4 and results are satisfactory\n",
    "# This will take ~5 minutes because we are not using the async option.\n",
    "def get_topics(group):\n",
    "    terms = ', '.join(list(group['term'].unique()))\n",
    "    topic, _, cost = get_completion(comment=terms, system_message=sm.identify_topic, force_json=False, model_class='gpt-3.5')\n",
    "    group['cluster_topic'] = topic\n",
    "    return group\n",
    "\n",
    "df = df.groupby('cluster', observed=True).apply(get_topics)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for the emotional valence, physicality, and humanity (see 'words_valence' prompt for defintions of these terms) of each cluster from gpt\n",
    "# We will use gpt-4 here because gpt-3.5 results were not satisfactory\n",
    "# This will take ~3 minutes because we are not using the async option.\n",
    "def get_scores(group):\n",
    "    terms = ', '.join(list(group['term'].unique()))\n",
    "    if group['cluster'].to_list()[0]>=0:\n",
    "        score, _, cost = get_completion(comment=terms, system_message=sm.words_valence, force_json=True, model_class='gpt-4')\n",
    "        try:\n",
    "            score = json.loads(score)\n",
    "            assert set(score.keys()).issubset({'valence', 'physicality', 'humanity'})\n",
    "        except:\n",
    "            print(f\"gpt gave us a bad score json for the topic {group['cluster_topic'].to_list()[0]}. It gave us:\")\n",
    "            print(score)\n",
    "            print('The scores for this topic will all be set to -1.')\n",
    "            score = {'valence': -1, 'physicality': -1, 'humanity': -1}\n",
    "    else: # we won't get valences for the seas\n",
    "        score = {'valence': 3, 'physicality': 3, 'humanity': 3}\n",
    "\n",
    "    try: #gpt can fail to return json with the expected keys\n",
    "        group['valence'] = score['valence']\n",
    "        group['physicality'] = score['physicality']\n",
    "        group['humanity'] = score['humanity']\n",
    "    except:\n",
    "        group['valence'] = 3\n",
    "        group['physicality'] = 3\n",
    "        group['humanity'] = 3\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby('cluster', observed=True).apply(get_scores)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fun name for the Land and Sea clusters from gpt.\n",
    "# This will take ~3 minutes because we are not using the async option.\n",
    "is_land = df.cluster.astype(int)>=0\n",
    "land_topics = df[is_land].cluster_topic.unique()\n",
    "topic_to_land_name = {topic: get_completion(comment=topic, system_message=sm.land_name, force_json=False, temperature=0.2, model_class='gpt-3.5')[0] for topic in land_topics}\n",
    "\n",
    "is_sea = df.cluster.astype(int)<0\n",
    "sea_topics = df[is_sea].cluster_topic.unique()\n",
    "topic_to_sea_name = {topic: get_completion(comment=topic, system_message=sm.sea_name, force_json=False, temperature=0.2, model_class='gpt-3.5')[0] for topic in sea_topics}\n",
    "\n",
    "df.loc[is_land, 'territory_name'] = df.loc[is_land, 'cluster_topic'].map(topic_to_land_name)\n",
    "df.loc[is_sea, 'territory_name'] = df.loc[is_sea, 'cluster_topic'].map(topic_to_sea_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally rename a cluster's territory name by hand here\n",
    "# df.loc[df.cluster==147, 'territory_name'] = 'The Land of Crisis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, filter out NSFW words\n",
    "with open('dirty-words-en.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "dirty_words = text.split('\\n')\n",
    "\n",
    "def is_dirty(word):\n",
    "    word = word.lower().strip(string.punctuation)\n",
    "    word_no_s = word[:-1] if word.endswith('s') else word # make the word singular\n",
    "    return word in dirty_words or word_no_s in dirty_words\n",
    "\n",
    "def replace_vowels_with_star(word):\n",
    "    return ''.join('*' if c in 'aeiouyAEIOUY' else c for c in word)\n",
    "\n",
    "def replace_center_letters(word):\n",
    "    return word[0] + '*' * (len(word)-2) + word[-1]\n",
    "\n",
    "def clean_word(word):\n",
    "    if is_dirty(word):\n",
    "        return replace_center_letters(word), True\n",
    "    return word, False\n",
    "\n",
    "def make_sfw(row):\n",
    "    sfw_word, word_is_nsfw  = clean_word(row.term)\n",
    "    sfw_definition = ' '.join([clean_word(word)[0] if clean_word(word)[1] else word for word in row.definition.split()])\n",
    "    definition_is_nsfw = sfw_definition != row.definition\n",
    "    return word_is_nsfw, definition_is_nsfw, sfw_word, sfw_definition\n",
    "\n",
    "df[['term_is_nsfw', 'definition_is_nsfw', 'sfw_term', 'sfw_definition']] = df[['term', 'definition']].apply(make_sfw, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster\"] = df[\"cluster\"].astype('category')\n",
    "df[\"valence\"] = df[\"valence\"].astype('category')\n",
    "df[\"physicality\"] = df[\"physicality\"].astype('category')\n",
    "df[\"humanity\"] = df[\"humanity\"].astype('category')\n",
    "\n",
    "df.to_parquet('semantics.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
